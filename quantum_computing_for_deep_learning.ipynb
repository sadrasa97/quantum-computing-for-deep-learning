{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TFXb3qoIw4HD",
        "outputId": "0fb5e43e-eb2b-4118-8e80-bbf476c93ef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: keras 2.15.0\n",
            "Uninstalling keras-2.15.0:\n",
            "  Successfully uninstalled keras-2.15.0\n",
            "Found existing installation: tensorflow 2.15.0\n",
            "Uninstalling tensorflow-2.15.0:\n",
            "  Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[33mWARNING: Skipping keras-nlp as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping autokeras as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.1.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-24.1.2\n",
            "Collecting tensorflow==2.9.1\n",
            "  Downloading tensorflow-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting keras==2.9.0\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.6.3)\n",
            "Collecting flatbuffers<2,>=1.12 (from tensorflow==2.9.1)\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl.metadata (872 bytes)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.9.1)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (3.9.0)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.9.1)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (24.1)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.9.1)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.16.0)\n",
            "Collecting tensorboard<2.10,>=2.9 (from tensorflow==2.9.1)\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (0.37.0)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.1)\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.1) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow==2.9.1)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.31.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.1)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.1)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.2.2)\n",
            "Downloading tensorflow-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, keras, flatbuffers, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, gast, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 24.3.25\n",
            "    Uninstalling flatbuffers-24.3.25:\n",
            "      Successfully uninstalled flatbuffers-24.3.25\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.15.0\n",
            "    Uninstalling tensorflow-estimator-2.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.0\n",
            "    Uninstalling google-auth-oauthlib-1.2.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-pubsub 2.21.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "googleapis-common-protos 1.63.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.6 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.19.6 which is incompatible.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.9.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "efed283a28ca454aa49f1c2f8e7f42a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.37.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting autokeras==1.0.20\n",
            "  Downloading autokeras-1.0.20-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting tensorflow-federated\n",
            "  Downloading tensorflow_federated-0.82.0-py3-none-manylinux_2_31_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting shap\n",
            "  Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
            "Collecting qiskit==0.39.0\n",
            "  Downloading qiskit-0.39.0.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting qiskit-aer==0.11.0\n",
            "  Downloading qiskit_aer-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting qiskit-terra==0.22.0\n",
            "  Downloading qiskit_terra-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.6 kB)\n",
            "Collecting qiskit-ibmq-provider==0.19.2\n",
            "  Downloading qiskit_ibmq_provider-0.19.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting qiskit-machine-learning==0.5.0\n",
            "  Downloading qiskit_machine_learning-0.5.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from autokeras==1.0.20) (24.1)\n",
            "Requirement already satisfied: tensorflow>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from autokeras==1.0.20) (2.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from autokeras==1.0.20) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.10/dist-packages (from qiskit-aer==0.11.0) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-aer==0.11.0) (1.11.4)\n",
            "Collecting retworkx>=0.11.0 (from qiskit-terra==0.22.0)\n",
            "  Downloading retworkx-0.15.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting ply>=3.10 (from qiskit-terra==0.22.0)\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl.metadata (844 bytes)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.22.0) (5.9.5)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.22.0) (1.12.1)\n",
            "Collecting dill>=0.3 (from qiskit-terra==0.22.0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra==0.22.0) (2.8.2)\n",
            "Collecting stevedore>=3.0.0 (from qiskit-terra==0.22.0)\n",
            "  Downloading stevedore-5.2.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting tweedledum<2.0,>=1.1 (from qiskit-terra==0.22.0)\n",
            "  Downloading tweedledum-1.1.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (10.0 kB)\n",
            "Collecting symengine>=0.9 (from qiskit-terra==0.22.0)\n",
            "  Downloading symengine-0.11.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: requests>=2.19 in /usr/local/lib/python3.10/dist-packages (from qiskit-ibmq-provider==0.19.2) (2.31.0)\n",
            "Collecting requests-ntlm>=1.1.0 (from qiskit-ibmq-provider==0.19.2)\n",
            "  Downloading requests_ntlm-1.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from qiskit-ibmq-provider==0.19.2) (2.0.7)\n",
            "Requirement already satisfied: websocket-client>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from qiskit-ibmq-provider==0.19.2) (1.8.0)\n",
            "Collecting websockets>=10.0 (from qiskit-ibmq-provider==0.19.2)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-machine-learning==0.5.0) (1.2.2)\n",
            "Collecting fastdtw (from qiskit-machine-learning==0.5.0)\n",
            "  Downloading fastdtw-0.3.4.tar.gz (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools>=40.1.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-machine-learning==0.5.0) (67.7.2)\n",
            "Collecting dill>=0.3 (from qiskit-terra==0.22.0)\n",
            "  Downloading dill-0.3.5.1-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.3)\n",
            "Collecting rustworkx (from pennylane)\n",
            "  Downloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.6.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting semantic-version>=2.7 (from pennylane)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting autoray>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.6.12-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.3.3)\n",
            "Collecting pennylane-lightning>=0.37 (from pennylane)\n",
            "  Downloading PennyLane_Lightning-0.37.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.12.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.9.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py==1.*,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (1.4.0)\n",
            "Requirement already satisfied: attrs~=23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (23.2.0)\n",
            "Requirement already satisfied: dm-tree==0.1.8 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (0.1.8)\n",
            "Collecting dp-accounting==0.4.3 (from tensorflow-federated)\n",
            "  Downloading dp_accounting-0.4.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting google-vizier==0.1.11 (from tensorflow-federated)\n",
            "  Downloading google_vizier-0.1.11-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: grpcio~=1.46 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (1.64.1)\n",
            "Collecting jaxlib==0.4.14 (from tensorflow-federated)\n",
            "  Downloading jaxlib-0.4.14-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting jax==0.4.14 (from tensorflow-federated)\n",
            "  Downloading jax-0.4.14.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting portpicker~=1.6 (from tensorflow-federated)\n",
            "  Downloading portpicker-1.6.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting scipy>=1.0 (from qiskit-aer==0.11.0)\n",
            "  Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-model-optimization==0.7.5 (from tensorflow-federated)\n",
            "  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl.metadata (914 bytes)\n",
            "Collecting tensorflow-privacy==0.9.0 (from tensorflow-federated)\n",
            "  Downloading tensorflow_privacy-0.9.0-py3-none-any.whl.metadata (763 bytes)\n",
            "Collecting tensorflow>=2.8.0 (from autokeras==1.0.20)\n",
            "  Downloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tqdm~=4.64 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (4.66.4)\n",
            "Collecting typing-extensions (from pennylane)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting googleapis-common-protos==1.61.0 (from tensorflow-federated)\n",
            "  Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: mpmath~=1.2 in /usr/local/lib/python3.10/dist-packages (from dp-accounting==0.4.3->tensorflow-federated) (1.3.0)\n",
            "Collecting attrs~=23.1 (from tensorflow-federated)\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: protobuf>=3.6 in /usr/local/lib/python3.10/dist-packages (from google-vizier==0.1.11->tensorflow-federated) (3.19.6)\n",
            "Collecting grpcio-tools>=1.35.0 (from google-vizier==0.1.11->tensorflow-federated)\n",
            "  Downloading grpcio_tools-1.65.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting sqlalchemy<=1.4.20,>=1.4 (from google-vizier==0.1.11->tensorflow-federated)\n",
            "  Downloading SQLAlchemy-1.4.20.tar.gz (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.14->tensorflow-federated) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax==0.4.14->tensorflow-federated) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras==1.0.20) (1.6.3)\n",
            "Collecting flatbuffers>=23.5.26 (from tensorflow>=2.8.0->autokeras==1.0.20)\n",
            "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras==1.0.20) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras==1.0.20) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras==1.0.20) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras==1.0.20) (18.1.1)\n",
            "Collecting protobuf>=3.6 (from google-vizier==0.1.11->tensorflow-federated)\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras==1.0.20) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras==1.0.20) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras==1.0.20) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.8.0->autokeras==1.0.20) (0.37.0)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow>=2.8.0->autokeras==1.0.20)\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow>=2.8.0->autokeras==1.0.20)\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras (from keras-tuner)\n",
            "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting packaging (from autokeras==1.0.20)\n",
            "  Downloading packaging-22.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting tensorflow-probability~=0.22.0 (from tensorflow-privacy==0.9.0->tensorflow-federated)\n",
            "  Downloading tensorflow_probability-0.22.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->qiskit-machine-learning==0.5.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->qiskit-machine-learning==0.5.0) (3.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lime) (3.7.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime) (0.19.3)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-24.4.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting slicer==0.0.8 (from shap)\n",
            "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19->qiskit-ibmq-provider==0.19.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19->qiskit-ibmq-provider==0.19.2) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19->qiskit-ibmq-provider==0.19.2) (2024.6.2)\n",
            "Requirement already satisfied: cryptography>=1.3 in /usr/local/lib/python3.10/dist-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.19.2) (42.0.8)\n",
            "Collecting pyspnego>=0.4.0 (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.19.2)\n",
            "  Downloading pyspnego-0.11.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2024.6.18)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (1.6.0)\n",
            "Collecting pbr!=2.1.0,>=2.0.0 (from stevedore>=3.0.0->qiskit-terra==0.22.0)\n",
            "  Downloading pbr-6.0.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd->pennylane) (0.18.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (3.1.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->autokeras==1.0.20) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->autokeras==1.0.20) (2024.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.8.0->autokeras==1.0.20) (0.43.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.19.2) (1.16.0)\n",
            "INFO: pip is looking at multiple versions of grpcio-tools to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-tools>=1.35.0 (from google-vizier==0.1.11->tensorflow-federated)\n",
            "  Downloading grpcio_tools-1.64.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.64.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<=1.4.20,>=1.4->google-vizier==0.1.11->tensorflow-federated) (3.0.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.8.0->autokeras==1.0.20) (2.27.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow>=2.8.0->autokeras==1.0.20)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.8.0->autokeras==1.0.20) (3.6)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.15,>=2.14->tensorflow>=2.8.0->autokeras==1.0.20)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.8.0->autokeras==1.0.20) (3.0.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow-federated) (4.4.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.19.2) (2.22)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow>=2.8.0->autokeras==1.0.20) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow>=2.8.0->autokeras==1.0.20) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow>=2.8.0->autokeras==1.0.20) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow>=2.8.0->autokeras==1.0.20) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow>=2.8.0->autokeras==1.0.20) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow>=2.8.0->autokeras==1.0.20) (3.2.2)\n",
            "Downloading autokeras-1.0.20-py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.4/162.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qiskit_aer-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qiskit_terra-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qiskit_ibmq_provider-0.19.2-py3-none-any.whl (240 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.4/240.4 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qiskit_machine_learning-0.5.0-py3-none-any.whl (152 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PennyLane-0.37.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_federated-0.82.0-py3-none-manylinux_2_31_x86_64.whl (71.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dp_accounting-0.4.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_vizier-0.1.11-py3-none-any.whl (721 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.6/721.6 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.9/230.9 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.4.14-cp310-cp310-manylinux2014_x86_64.whl (73.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.9/489.9 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_privacy-0.9.0-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.2/323.2 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (540 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.1/540.1 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
            "Downloading autoray-0.6.12-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-22.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PennyLane_Lightning-0.37.0-cp310-cp310-manylinux_2_28_x86_64.whl (15.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portpicker-1.6.0-py3-none-any.whl (16 kB)\n",
            "Downloading pyaml-24.4.0-py3-none-any.whl (24 kB)\n",
            "Downloading requests_ntlm-1.3.0-py3-none-any.whl (6.6 kB)\n",
            "Downloading retworkx-0.15.1-py3-none-any.whl (10 kB)\n",
            "Downloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.7/33.7 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading stevedore-5.2.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading symengine-0.11.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (39.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.4/39.4 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tweedledum-1.1.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (929 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m929.7/929.7 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
            "Downloading grpcio_tools-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pbr-6.0.0-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.5/107.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyspnego-0.11.0-py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.4/130.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_probability-0.22.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: qiskit, jax, lime, fastdtw, sqlalchemy\n",
            "  Building wheel for qiskit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for qiskit: filename=qiskit-0.39.0-py3-none-any.whl size=12164 sha256=73762804b90caaa5818f132726a740aa3fe5aa323e71d39dc6b423f90a7fa8d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/45/03/d288ac15e6ef3467a77ff2a7757a126ea0aecfb440f871086f\n",
            "  Building wheel for jax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.4.14-py3-none-any.whl size=1535361 sha256=a7396443344760ef4d0b406ef2f037793d5ff6cf9f2635f8d14904fa8616649f\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/52/e7/dfa571c9f9b879e3facaa1584f52be04c4c3d1e14054ef40ab\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283835 sha256=a7d37f2b69ed786e50a543166b16185dc38c8c23f6bb46606413707e80499025\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/a2/af/9ac0a1a85a27f314a06b39e1f492bee1547d52549a4606ed89\n",
            "  Building wheel for fastdtw (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastdtw: filename=fastdtw-0.3.4-cp310-cp310-linux_x86_64.whl size=512613 sha256=13396428531d6e6fb61309ce610e20199c77a871d18f2f7cdc0720ff8a59fde7\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/c8/f7/c25448dab74c3acf4848bc25d513c736bb93910277e1528ef4\n",
            "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlalchemy: filename=SQLAlchemy-1.4.20-cp310-cp310-linux_x86_64.whl size=1529862 sha256=672ca0351b08e312c30188eba3dabc51051cf2a8cde57fd49a82925ad2144e3c\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/42/20/a958989c470cc1a6fe1d1279b0193f0e508161327fc3d951d9\n",
            "Successfully built qiskit jax lime fastdtw sqlalchemy\n",
            "Installing collected packages: ply, kt-legacy, flatbuffers, appdirs, websockets, typing-extensions, tweedledum, tensorflow-probability, tensorflow-model-optimization, tensorflow-estimator, tensorboard-data-server, symengine, sqlalchemy, slicer, semantic-version, scipy, rustworkx, pyaml, protobuf, portpicker, pbr, packaging, keras, fastdtw, dill, autoray, attrs, stevedore, retworkx, keras-tuner, jaxlib, jax, grpcio-tools, googleapis-common-protos, dp-accounting, shap, scikit-optimize, qiskit-terra, pyspnego, lime, google-vizier, google-auth-oauthlib, tensorboard, requests-ntlm, qiskit-machine-learning, qiskit-aer, tensorflow, qiskit-ibmq-provider, tensorflow-privacy, qiskit, autokeras, tensorflow-federated, pennylane-lightning, pennylane\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.23.0\n",
            "    Uninstalling tensorflow-probability-0.23.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.23.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.6.1\n",
            "    Uninstalling tensorboard-data-server-0.6.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.31\n",
            "    Uninstalling SQLAlchemy-2.0.31:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.31\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "  Attempting uninstall: portpicker\n",
            "    Found existing installation: portpicker 1.5.2\n",
            "    Uninstalling portpicker-1.5.2:\n",
            "      Successfully uninstalled portpicker-1.5.2\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.1\n",
            "    Uninstalling packaging-24.1:\n",
            "      Successfully uninstalled packaging-24.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 23.2.0\n",
            "    Uninstalling attrs-23.2.0:\n",
            "      Successfully uninstalled attrs-23.2.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.26+cuda12.cudnn89\n",
            "    Uninstalling jaxlib-0.4.26+cuda12.cudnn89:\n",
            "      Successfully uninstalled jaxlib-0.4.26+cuda12.cudnn89\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.26\n",
            "    Uninstalling jax-0.4.26:\n",
            "      Successfully uninstalled jax-0.4.26\n",
            "  Attempting uninstall: googleapis-common-protos\n",
            "    Found existing installation: googleapis-common-protos 1.63.2\n",
            "    Uninstalling googleapis-common-protos-1.63.2:\n",
            "      Successfully uninstalled googleapis-common-protos-1.63.2\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.1\n",
            "    Uninstalling tensorflow-2.9.1:\n",
            "      Successfully uninstalled tensorflow-2.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.3.0+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "chex 0.1.86 requires jax>=0.4.16, but you have jax 0.4.14 which is incompatible.\n",
            "flax 0.8.4 requires jax>=0.4.19, but you have jax 0.4.14 which is incompatible.\n",
            "google-colab 1.0.0 requires portpicker==1.5.2, but you have portpicker 1.6.0 which is incompatible.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.20 which is incompatible.\n",
            "pydantic 2.8.0 requires typing-extensions>=4.6.1; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.20.0 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.3 which is incompatible.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.14.1 which is incompatible.\n",
            "torch 2.3.0+cu121 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 attrs-23.1.0 autokeras-1.0.20 autoray-0.6.12 dill-0.3.5.1 dp-accounting-0.4.3 fastdtw-0.3.4 flatbuffers-24.3.25 google-auth-oauthlib-1.0.0 google-vizier-0.1.11 googleapis-common-protos-1.61.0 grpcio-tools-1.62.2 jax-0.4.14 jaxlib-0.4.14 keras-2.14.0 keras-tuner-1.4.7 kt-legacy-1.0.5 lime-0.2.0.1 packaging-22.0 pbr-6.0.0 pennylane-0.37.0 pennylane-lightning-0.37.0 ply-3.11 portpicker-1.6.0 protobuf-4.25.3 pyaml-24.4.0 pyspnego-0.11.0 qiskit-0.39.0 qiskit-aer-0.11.0 qiskit-ibmq-provider-0.19.2 qiskit-machine-learning-0.5.0 qiskit-terra-0.22.0 requests-ntlm-1.3.0 retworkx-0.15.1 rustworkx-0.15.1 scikit-optimize-0.10.2 scipy-1.9.3 semantic-version-2.10.0 shap-0.46.0 slicer-0.0.8 sqlalchemy-1.4.20 stevedore-5.2.0 symengine-0.11.0 tensorboard-2.14.1 tensorboard-data-server-0.7.2 tensorflow-2.14.1 tensorflow-estimator-2.14.0 tensorflow-federated-0.82.0 tensorflow-model-optimization-0.7.5 tensorflow-privacy-0.9.0 tensorflow-probability-0.22.1 tweedledum-1.1.1 typing-extensions-4.5.0 websockets-12.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "portpicker"
                ]
              },
              "id": "19ab0d191ce0486f8e1e15c7640a773c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip uninstall -y keras tensorflow keras-nlp autokeras\n",
        "!pip install --upgrade pip\n",
        "!pip install tensorflow==2.9.1 keras==2.9.0\n",
        "!pip install pennylane autokeras==1.0.20 keras-tuner tensorflow-federated lime scikit-optimize shap qiskit==0.39.0 qiskit-aer==0.11.0 qiskit-terra==0.22.0 qiskit-ibmq-provider==0.19.2 qiskit-machine-learning==0.5.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1KTsGyZw48R",
        "outputId": "1ed65173-f6ce-46eb-dd33-b75366d37937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:qiskit_aer.noise.noise_model:WARNING: all-qubit error already exists for instruction \"u1\", composing with additional error.\n",
            "WARNING:qiskit_aer.noise.noise_model:WARNING: all-qubit error already exists for instruction \"u2\", composing with additional error.\n",
            "WARNING:qiskit_aer.noise.noise_model:WARNING: all-qubit error already exists for instruction \"u3\", composing with additional error.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground state energy: -2.1881368228947355\n",
            "Quantum circuit execution result: -0.12827212485461847\n",
            "Processing MNIST with Quantum SVM\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Real, Integer\n",
        "from skopt.utils import use_named_args\n",
        "import matplotlib.pyplot as plt\n",
        "import keras_tuner as kt\n",
        "import autokeras as ak\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.datasets import mnist, fashion_mnist, cifar10\n",
        "from qiskit import Aer, transpile, execute, QuantumCircuit\n",
        "from qiskit.circuit.library import ZZFeatureMap, TwoLocal\n",
        "from qiskit.circuit import ParameterVector\n",
        "from qiskit.algorithms import VQE\n",
        "from qiskit.algorithms.optimizers import COBYLA\n",
        "from qiskit.opflow import I, X, Z\n",
        "from qiskit.providers.aer.noise import NoiseModel, depolarizing_error, thermal_relaxation_error\n",
        "from qiskit.providers.aer import AerSimulator\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.datasets import mnist, fashion_mnist, cifar10\n",
        "from qiskit import Aer, transpile, execute, QuantumCircuit\n",
        "from qiskit.circuit.library import ZZFeatureMap, TwoLocal\n",
        "from qiskit.circuit import ParameterVector\n",
        "from qiskit.algorithms import VQE\n",
        "from qiskit.algorithms.optimizers import COBYLA\n",
        "from qiskit.opflow import I, X, Z\n",
        "from qiskit.providers.aer.noise import NoiseModel, depolarizing_error, thermal_relaxation_error\n",
        "from qiskit.providers.aer import AerSimulator\n",
        "import time\n",
        "from qiskit.circuit.library import ZZFeatureMap\n",
        "from qiskit import Aer, execute, QuantumCircuit\n",
        "from qiskit.utils import QuantumInstance\n",
        "from qiskit import Aer\n",
        "from qiskit.circuit.library import ZZFeatureMap\n",
        "from qiskit.utils import QuantumInstance\n",
        "from qiskit_machine_learning.kernels import QuantumKernel\n",
        "from sklearn.decomposition import PCA\n",
        "from qiskit_machine_learning.kernels import QuantumKernel\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "def create_quantum_kernel(feature_map):\n",
        "    backend = Aer.get_backend('qasm_simulator')\n",
        "    quantum_instance = QuantumInstance(backend)\n",
        "    kernel = QuantumKernel(feature_map=feature_map, quantum_instance=quantum_instance)\n",
        "    return kernel\n",
        "\n",
        "# Quantum SVM training and evaluation\n",
        "def quantum_svm_training(train_images, train_labels, test_images, test_labels):\n",
        "    feature_map = ZZFeatureMap(feature_dimension=train_images.shape[1], reps=2, entanglement='linear')\n",
        "    quantum_kernel = create_quantum_kernel(feature_map)\n",
        "\n",
        "    # Train the SVM with quantum kernel\n",
        "    qsvc = SVC(kernel=quantum_kernel.evaluate)\n",
        "    qsvc.fit(train_images, train_labels)\n",
        "\n",
        "    # Evaluate the SVM with quantum kernel\n",
        "    qsvc_predictions = qsvc.predict(test_images)\n",
        "    evaluate_model(qsvc_predictions, test_labels)\n",
        "    return qsvc\n",
        "\n",
        "# Function to perform Quantum Kernel PCA\n",
        "def qpca_analysis(train_images, n_components=2):\n",
        "    feature_map = ZZFeatureMap(feature_dimension=train_images.shape[1], reps=2, entanglement='linear')\n",
        "    quantum_kernel = create_quantum_kernel(feature_map)\n",
        "\n",
        "    # Compute the kernel matrix\n",
        "    kernel_matrix = quantum_kernel.evaluate(train_images)\n",
        "\n",
        "    # Apply classical PCA to the kernel matrix\n",
        "    pca = PCA(n_components=n_components)\n",
        "    reduced_data = pca.fit_transform(kernel_matrix)\n",
        "\n",
        "    return reduced_data\n",
        "\n",
        "\n",
        "def preprocess_data(images, labels, class1, class2, img_shape):\n",
        "    images = images.reshape(-1, img_shape) / 255.0\n",
        "    scaler = StandardScaler()\n",
        "    images = scaler.fit_transform(images)\n",
        "    images = images[(labels == class1) | (labels == class2)]\n",
        "    labels = labels[(labels == class1) | (labels == class2)]\n",
        "    labels = np.where(labels == class1, -1, 1)\n",
        "    return images, labels\n",
        "\n",
        "# Load datasets\n",
        "datasets = {\n",
        "    \"MNIST\": mnist.load_data(),\n",
        "    \"Fashion MNIST\": fashion_mnist.load_data(),\n",
        "    \"CIFAR-10\": cifar10.load_data()\n",
        "}\n",
        "\n",
        "class1 = 0\n",
        "class2 = 1\n",
        "\n",
        "# VQE and Quantum Circuit Definitions\n",
        "n_qubits = 4\n",
        "n_layers = 10\n",
        "backend = Aer.get_backend('statevector_simulator')\n",
        "\n",
        "# Define Hamiltonian for VQE\n",
        "H = (Z ^ I) + (I ^ Z) + 0.5 * (X ^ X) + 0.5 * (I ^ X)\n",
        "H = H.reduce()\n",
        "\n",
        "# Classical optimizer for VQE\n",
        "optimizer = COBYLA(maxiter=100)\n",
        "\n",
        "# Variational form for VQE\n",
        "var_form = TwoLocal(rotation_blocks=['ry', 'rz'], entanglement_blocks='cz')\n",
        "\n",
        "# Create a VQE instance\n",
        "vqe = VQE(var_form, optimizer, quantum_instance=backend)\n",
        "\n",
        "# Run VQE to find the ground state energy\n",
        "result = vqe.compute_minimum_eigenvalue(H)\n",
        "print(\"Ground state energy:\", result.eigenvalue.real)\n",
        "\n",
        "# Define quantum feature mapping\n",
        "feature_map = ZZFeatureMap(feature_dimension=n_qubits, reps=2, entanglement='linear')\n",
        "\n",
        "# Define noise model\n",
        "noise_model = NoiseModel()\n",
        "error_1 = depolarizing_error(0.01, 1)\n",
        "error_2 = depolarizing_error(0.1, 2)  # Define a two-qubit depolarizing error\n",
        "thermal_relaxation_single = thermal_relaxation_error(50, 70, 1)\n",
        "\n",
        "noise_model.add_all_qubit_quantum_error(error_1, ['u1', 'u2', 'u3'])\n",
        "noise_model.add_all_qubit_quantum_error(thermal_relaxation_single, ['u1', 'u2', 'u3'])\n",
        "noise_model.add_all_qubit_quantum_error(error_2, ['cx'])\n",
        "\n",
        "# Create a noisy simulator backend\n",
        "noisy_backend = AerSimulator(noise_model=noise_model)\n",
        "\n",
        "# Function to create and execute quantum circuit\n",
        "def create_quantum_circuit(weights, x):\n",
        "    circuit = QuantumCircuit(n_qubits, name='quantum_circuit')\n",
        "    feature_params = ParameterVector('x', n_qubits)\n",
        "\n",
        "    # Add feature map with unique parameters\n",
        "    feature_map_circuit = feature_map.assign_parameters({feature_map.parameters[i]: feature_params[i] for i in range(n_qubits)})\n",
        "    circuit.compose(feature_map_circuit, inplace=True)\n",
        "\n",
        "    for layer_weights in weights:\n",
        "        for i in range(n_qubits):\n",
        "            circuit.rx(float(layer_weights[i, 0]), i)\n",
        "            circuit.ry(float(layer_weights[i, 1]), i)\n",
        "            circuit.rz(float(layer_weights[i, 2]), i)\n",
        "        for i in range(n_qubits):\n",
        "            circuit.cx(i, (i + 1) % n_qubits)\n",
        "\n",
        "    circuit.save_statevector('final_statevector')  # Use a unique label\n",
        "    return circuit, feature_params\n",
        "\n",
        "def quantum_circuit_execution(kernel, x):\n",
        "    circuit, feature_params = create_quantum_circuit(kernel, x)\n",
        "    param_dict = {feature_params[i]: float(x[i]) for i in range(n_qubits)}\n",
        "\n",
        "    bound_circuit = circuit.assign_parameters(param_dict)\n",
        "    transpiled_circuit = transpile(bound_circuit, backend)\n",
        "    job = execute(transpiled_circuit, backend)\n",
        "\n",
        "    while job.status().name not in ['DONE', 'ERROR', 'CANCELLED']:\n",
        "        time.sleep(1)\n",
        "\n",
        "    result = job.result()\n",
        "    experiment_name = result.results[0].header.name\n",
        "    statevector = result.data(experiment_name)['final_statevector']\n",
        "    return np.real(statevector[0])\n",
        "\n",
        "# Test the quantum circuit execution\n",
        "weights = np.random.uniform(size=(n_layers, n_qubits, 3))\n",
        "x = np.random.uniform(size=(n_qubits,))\n",
        "result = quantum_circuit_execution(weights, x)\n",
        "print(\"Quantum circuit execution result:\", result)\n",
        "# Define QuantumLayer class\n",
        "class QuantumLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, n_qubits, n_layers, backend, feature_map, **kwargs):\n",
        "        super(QuantumLayer, self).__init__(**kwargs)\n",
        "        self.n_qubits = n_qubits\n",
        "        self.n_layers = n_layers\n",
        "        self.backend = backend\n",
        "        self.feature_map = feature_map\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(name='kernel',\n",
        "                                      shape=(self.n_layers, self.n_qubits, 3),\n",
        "                                      initializer='uniform',\n",
        "                                      trainable=True)\n",
        "        super(QuantumLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        def execute_quantum_circuit(inputs, kernel):\n",
        "            inputs_np = inputs.numpy()  # Convert inputs to numpy array\n",
        "            result = quantum_circuit_execution(kernel, inputs_np)\n",
        "            return result\n",
        "\n",
        "        result = tf.map_fn(lambda x: tf.py_function(execute_quantum_circuit, [x, self.kernel], tf.float32), inputs)\n",
        "        result = tf.reshape(result, [-1, 1])  # Ensure the result is reshaped to the correct output shape\n",
        "        return result\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 1)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Optimizer\n",
        "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
        "import numpy as np\n",
        "\n",
        "class QuantumGradientDescent(Optimizer):\n",
        "    def __init__(self, learning_rate, name=\"QuantumGradientDescent\", **kwargs):\n",
        "        super().__init__(name, **kwargs)\n",
        "        if isinstance(learning_rate, (float, int)):\n",
        "            self._learning_rate = learning_rate\n",
        "        elif isinstance(learning_rate, LearningRateSchedule):\n",
        "            self._learning_rate = learning_rate\n",
        "        else:\n",
        "            raise ValueError(\"Invalid type for learning_rate. Expected float, int, or LearningRateSchedule.\")\n",
        "\n",
        "    @property\n",
        "    def learning_rate(self):\n",
        "        return self._learning_rate\n",
        "\n",
        "    def _create_slots(self, var_list):\n",
        "        for var in var_list:\n",
        "            self.add_slot(var, 'momentum')\n",
        "\n",
        "    def _resource_apply_dense(self, grad, var, apply_state=None):\n",
        "        var_dtype = var.dtype.base_dtype\n",
        "        momentum = self.get_slot(var, 'momentum')\n",
        "        lr = self.learning_rate\n",
        "        new_momentum = momentum.assign(momentum * 0.9 + grad)\n",
        "        var.assign_sub(lr * new_momentum)\n",
        "\n",
        "    def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n",
        "        var_dtype = var.dtype.base_dtype\n",
        "        momentum = self.get_slot(var, 'momentum')\n",
        "        lr = self.learning_rate\n",
        "        new_momentum = momentum.assign(momentum * 0.9 + grad)\n",
        "        var.assign_sub(lr * new_momentum)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"learning_rate\": self.learning_rate,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "tf.keras.optimizers.QuantumGradientDescent = QuantumGradientDescent\n",
        "\n",
        "# Define the Quantum Gradient Descent optimizer\n",
        "qgd_optimizer = QuantumGradientDescent(learning_rate=0.01)\n",
        "\n",
        "def build_hybrid_nn(input_shape, n_qubits, n_layers, backend, feature_map):\n",
        "    inputs = layers.Input(shape=(input_shape,))\n",
        "    quantum_output = QuantumLayer(n_qubits, n_layers, backend, feature_map)(inputs)\n",
        "    outputs = layers.Dense(1, activation='linear')(quantum_output)\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Updated cross-validation and training code with debugging steps\n",
        "def train_custom_model(model, data, labels, epochs, batch_size):\n",
        "    optimizer = QuantumGradientDescent(learning_rate=0.001)\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((data, labels)).batch(batch_size)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f'Start of epoch {epoch+1}')\n",
        "        for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "            with tf.GradientTape() as tape:\n",
        "                logits = model(x_batch, training=True)\n",
        "                loss_value = loss_fn(y_batch, logits)\n",
        "\n",
        "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "            # Debugging shapes\n",
        "            print(f'Step {step+1}, x_batch shape: {x_batch.shape}, logits shape: {logits.shape}, loss: {loss_value.numpy()}')\n",
        "\n",
        "        print(f'Epoch {epoch+1} completed')\n",
        "\n",
        "def cross_validate_model(data, labels, input_shape, n_qubits, n_layers, backend, feature_map, k=5, epochs=20, batch_size=32):\n",
        "    print(f'Data shape: {data.shape}, Labels shape: {labels.shape}')\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "    val_scores = []\n",
        "\n",
        "    for train_index, val_index in kf.split(data):\n",
        "        train_data, val_data = data[train_index], data[val_index]\n",
        "        train_labels, val_labels = labels[train_index], labels[val_index]\n",
        "\n",
        "        print(f'Train data shape: {train_data.shape}, Train labels shape: {train_labels.shape}')\n",
        "        print(f'Val data shape: {val_data.shape}, Val labels shape: {val_labels.shape}')\n",
        "\n",
        "        model = build_hybrid_nn(input_shape, n_qubits, n_layers, backend, feature_map)\n",
        "        train_custom_model(model, train_data, train_labels, epochs, batch_size)\n",
        "        val_logits = model.predict(val_data)\n",
        "        val_loss = tf.keras.losses.MeanSquaredError()(val_labels, val_logits).numpy()\n",
        "        val_scores.append(val_loss)\n",
        "\n",
        "    print(f'Cross-Validation Mean Squared Error: {np.mean(val_scores)}')\n",
        "for dataset_name, (train_data, test_data) in datasets.items():\n",
        "    print(f\"Processing {dataset_name} with Quantum SVM\")\n",
        "    (train_images, train_labels) = train_data\n",
        "    (test_images, test_labels) = test_data\n",
        "    img_shape = np.prod(train_images.shape[1:])\n",
        "    train_images, train_labels = preprocess_data(train_images, train_labels, class1, class2, img_shape)\n",
        "    test_images, test_labels = preprocess_data(test_images, test_labels, class1, class2, img_shape)\n",
        "    qsvc_model = quantum_svm_training(train_images, train_labels, test_images, test_labels)\n",
        "    reduced_data = qpca_analysis(train_images)\n",
        "    print(f\"{dataset_name} Reduced data shape:\", reduced_data.shape)\n",
        "# Cross-validation\n",
        "for dataset_name, (train_data, test_data) in datasets.items():\n",
        "    print(f\"Processing {dataset_name} with Cross-Validation\")\n",
        "\n",
        "    (train_images, train_labels), (test_images, test_labels) = train_data, test_data\n",
        "    img_shape = np.prod(train_images.shape[1:])\n",
        "    train_images, train_labels = preprocess_data(train_images, train_labels, class1, class2, img_shape)\n",
        "\n",
        "    input_shape = train_images.shape[1]\n",
        "    cross_validate_model(train_images, train_labels, input_shape, n_qubits, n_layers, backend, feature_map)\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(predictions, labels):\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision = precision_score(labels, predictions)\n",
        "    recall = recall_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions)\n",
        "    roc_auc = roc_auc_score(labels, predictions)\n",
        "    logloss = log_loss(labels, predictions)\n",
        "\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print(f'Precision: {precision}')\n",
        "    print(f'Recall: {recall}')\n",
        "    print(f'F1 Score: {f1}')\n",
        "    print(f'ROC AUC: {roc_auc}')\n",
        "    print(f'Log Loss: {logloss}')\n",
        "\n",
        "# Function to plot confusion matrix\n",
        "def plot_confusion_matrix(cm, title):\n",
        "    fig, ax = plt.subplots()\n",
        "    cax = ax.matshow(cm, cmap='Blues')\n",
        "    plt.title(title)\n",
        "    fig.colorbar(cax)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n",
        "\n",
        "# Regularized neural network model\n",
        "def build_regularized_nn(optimizer, input_shape):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(256, activation='relu', input_shape=(input_shape,), kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(1, activation='tanh'))\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Function to plot training history\n",
        "def plot_history(history, title):\n",
        "    fig, ax1 = plt.subplots()\n",
        "\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Loss', color='tab:blue')\n",
        "    ax1.plot(history.history['loss'], label='Training Loss', color='tab:blue')\n",
        "    ax1.plot(history.history['val_loss'], label='Validation Loss', color='tab:blue', linestyle='dashed')\n",
        "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.set_ylabel('Accuracy', color='tab:orange')\n",
        "    ax2.plot(history.history['accuracy'], label='Training Accuracy', color='tab:orange')\n",
        "    ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', color='tab:orange', linestyle='dashed')\n",
        "    ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.title(title)\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "# Grid Search for hyperparameter tuning\n",
        "def grid_search(model, param_grid, train_images, train_labels):\n",
        "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "    grid_result = grid.fit(train_images, train_labels)\n",
        "\n",
        "    print(f'Best Parameters: {grid_result.best_params_}')\n",
        "    print(f'Best Score: {grid_result.best_score_}')\n",
        "    return grid_result.best_estimator_\n",
        "\n",
        "# Define a parameter grid for Grid Search\n",
        "param_grid = {\n",
        "    'batch_size': [32, 64, 128],\n",
        "    'epochs': [10, 20, 30],\n",
        "    'optimizer': ['adam', 'sgd']\n",
        "}\n",
        "\n",
        "# Hyperparameter optimization function\n",
        "space = [\n",
        "    Real(0.01, 0.5, name='learning_rate'),\n",
        "    Integer(32, 128, name='batch_size')\n",
        "]\n",
        "\n",
        "@use_named_args(space)\n",
        "def objective(params):\n",
        "    learning_rate = params[0]\n",
        "    batch_size = params[1]\n",
        "\n",
        "    model = build_regularized_nn(optimizers.Adam(learning_rate=learning_rate), img_shape)\n",
        "    history = model.fit(\n",
        "        datagen.flow(train_images.reshape(-1, 28, 28, 1) if dataset_name != \"CIFAR-10\" else train_images.reshape(-1, 32, 32, 3), train_labels, batch_size=batch_size),\n",
        "        epochs=20,\n",
        "        validation_data=(val_images, val_labels),\n",
        "        verbose=0,\n",
        "        callbacks=[early_stopping, lr_scheduler]\n",
        "    )\n",
        "    val_loss = np.min(history.history['val_loss'])\n",
        "    return val_loss\n",
        "\n",
        "# Running the hyperparameter optimization\n",
        "res_gp = gp_minimize(objective, space, n_calls=20, random_state=0)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_learning_rate = res_gp.x[0]\n",
        "best_batch_size = res_gp.x[1]\n",
        "\n",
        "print(f'Best learning rate: {best_learning_rate}')\n",
        "print(f'Best batch size: {best_batch_size}')\n",
        "\n",
        "# Hyperband tuner for Neural Architecture Search\n",
        "def hyperband_tuner(train_images, train_labels, img_shape):\n",
        "    def build_model(hp):\n",
        "        model = models.Sequential()\n",
        "        model.add(layers.Dense(\n",
        "            units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
        "            activation='relu',\n",
        "            input_shape=(img_shape,)\n",
        "        ))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.Dropout(0.5))\n",
        "        model.add(layers.Dense(\n",
        "            units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
        "            activation='relu'\n",
        "        ))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.Dropout(0.5))\n",
        "        model.add(layers.Dense(1, activation='tanh'))\n",
        "        model.compile(\n",
        "            optimizer=optimizers.Adam(\n",
        "                hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "            ),\n",
        "            loss='mean_squared_error',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    tuner = kt.Hyperband(\n",
        "        build_model,\n",
        "        objective='val_accuracy',\n",
        "        max_epochs=20,\n",
        "        factor=3,\n",
        "        directory='my_dir',\n",
        "        project_name='intro_to_kt'\n",
        "    )\n",
        "\n",
        "    tuner.search(train_images, train_labels, epochs=50, validation_split=0.2)\n",
        "\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "    return best_model\n",
        "\n",
        "# AutoML with AutoKeras\n",
        "def auto_ml(train_images, train_labels, test_images, test_labels):\n",
        "    auto_model = ak.ImageClassifier(overwrite=True, max_trials=3)\n",
        "    auto_model.fit(train_images, train_labels, epochs=20)\n",
        "    results = auto_model.evaluate(test_images, test_labels)\n",
        "    print(f'AutoML Model Accuracy on {dataset_name}: {results[1]}')\n",
        "    return auto_model\n",
        "\n",
        "# Main code\n",
        "datasets = load_datasets()\n",
        "class1, class2 = 0, 1\n",
        "\n",
        "for dataset_name, (train_data, test_data) in datasets.items():\n",
        "    print(f\"Processing {dataset_name} with Grid Search\")\n",
        "\n",
        "    (train_images, train_labels) = train_data\n",
        "    (test_images, test_labels) = test_data\n",
        "    img_shape = 28 * 28 if dataset_name != \"CIFAR-10\" else 32 * 32 * 3\n",
        "\n",
        "    train_images, train_labels = preprocess_data(train_images, train_labels, class1, class2, img_shape)\n",
        "    test_images, test_labels = preprocess_data(test_images, test_labels, class1, class2, img_shape)\n",
        "\n",
        "    model = build_regularized_nn(optimizers.Adam(), img_shape)\n",
        "    best_model = grid_search(model, param_grid, train_images, train_labels)\n",
        "\n",
        "    predictions = np.argmax(best_model.predict(test_images), axis=1)\n",
        "    accuracy = accuracy_score(test_labels, predictions)\n",
        "    print(f'Grid Search Best Model Accuracy on {dataset_name}: {accuracy}')\n",
        "\n",
        "for dataset_name, (train_data, test_data) in datasets.items():\n",
        "    print(f\"Processing {dataset_name} with Hyperband Tuner\")\n",
        "\n",
        "    (train_images, train_labels) = train_data\n",
        "    (test_images, test_labels) = test_data\n",
        "    img_shape = 28 * 28 if dataset_name != \"CIFAR-10\" else 32 * 32 * 3\n",
        "\n",
        "    train_images, train_labels = preprocess_data(train_images, train_labels, class1, class2, img_shape)\n",
        "    test_images, test_labels = preprocess_data(test_images, test_labels, class1, class2, img_shape)\n",
        "\n",
        "    best_model = hyperband_tuner(train_images, train_labels, img_shape)\n",
        "    results = best_model.evaluate(test_images, test_labels)\n",
        "    print(f'Hyperband Tuner Best Model Accuracy on {dataset_name}: {results[1]}')\n",
        "\n",
        "for dataset_name, (train_data, test_data) in datasets.items():\n",
        "    print(f\"Processing {dataset_name} with AutoML\")\n",
        "\n",
        "    (train_images, train_labels) = train_data\n",
        "    (test_images, test_labels) = test_data\n",
        "    img_shape = 28 * 28 if dataset_name != \"CIFAR-10\" else 32 * 32 * 3\n",
        "\n",
        "    train_images, train_labels = preprocess_data(train_images, train_labels, class1, class2, img_shape)\n",
        "    test_images, test_labels = preprocess_data(test_images, test_labels, class1, class2, img_shape)\n",
        "\n",
        "    auto_ml_model = auto_ml(train_images, train_labels, test_images, test_labels)\n",
        "\n",
        "# Evaluate models\n",
        "for dataset_name, (train_data, test_data) in datasets.items():\n",
        "    print(f\"Evaluating models for {dataset_name}\")\n",
        "\n",
        "    (train_images, train_labels) = train_data\n",
        "    (test_images, test_labels) = test_data\n",
        "    img_shape = 28 * 28 if dataset_name != \"CIFAR-10\" else 32 * 32 * 3\n",
        "\n",
        "    train_images, train_labels = preprocess_data(train_images, train_labels, class1, class2, img_shape)\n",
        "    test_images, test_labels = preprocess_data(test_images, test_labels, class1, class2, img_shape)\n",
        "\n",
        "    # Evaluate Quantum Gradient Descent model\n",
        "    qgd_predictions = [quantum_circuit_execution(weights, x) for x in test_images]\n",
        "    qgd_predictions = np.sign(qgd_predictions)\n",
        "    print(f'Quantum Gradient Descent Evaluation for {dataset_name}')\n",
        "    evaluate_model(qgd_predictions, test_labels)\n",
        "\n",
        "    # Evaluate Gradient Descent model\n",
        "    sgd_predictions = np.argmax(model_sgd.predict(test_images), axis=1)\n",
        "    print(f' Gradient Descent Evaluation for {dataset_name}')\n",
        "    evaluate_model(sgd_predictions, test_labels)\n",
        "\n",
        "    # Evaluate Adam optimizer model\n",
        "    adam_predictions = np.argmax(model_adam.predict(test_images), axis=1)\n",
        "    print(f'Adam Optimizer Evaluation for {dataset_name}')\n",
        "    evaluate_model(adam_predictions, test_labels)\n",
        "\n",
        "    # Confusion matrices\n",
        "    cm_qgd = confusion_matrix(test_labels, qgd_predictions)\n",
        "    cm_sgd = confusion_matrix(test_labels, sgd_predictions)\n",
        "    cm_adam = confusion_matrix(test_labels, adam_predictions)\n",
        "\n",
        "    plot_confusion_matrix(cm_qgd, f'QGD Confusion Matrix for {dataset_name}')\n",
        "    plot_confusion_matrix(cm_sgd, f'SGD Confusion Matrix for {dataset_name}')\n",
        "    plot_confusion_matrix(cm_adam, f'Adam Confusion Matrix for {dataset_name}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}